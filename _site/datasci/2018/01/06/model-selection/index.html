<!DOCTYPE html> <html> <head> <title>Michael Ge | Software Engineer | Graphic Artist</title> <meta charset="UTF-8"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-112248037-1"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-112248037-1");</script> <link rel="stylesheet" type="text/css" href="/assets/css/post.css"> <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico"> </head> <body> <div id="nav-container"> <ul class="nav"> <li class="navli"><a class="text nava" href="/">HOME</a></li> <li class="navli"><a class="text nava" href="/about">ABOUT ME</a></li> <li class="navli"><a class="text nava" href="/art">ART</a></li> <li class="navli"><a class="text nava" href="/code">CODE</a></li> <li class="navli"><a class="text nava" href="/blog">BLOG</a></li> <li class="navli"><a class="text nava" href="/contact">CONTACT</a></li> </ul> </div> <div class="spacer"></div> <div class="blog-container"> <h1 class="header">Model Selection</h1> <p class="invite">01/06/2018</p> <p class="desc tags"><i>Tags: machine learning model selection overfitting </i></p> <p>So far, we’ve looked at linear regression and K-Nearest Neighbors as potential models for estimating real-valued data. But how do we know which model is the best to use? In this post, we discuss overfitting, bias-variance decomposition, and regularization as factors when considering models.</p> <h1 id="naive-solution">Naive Solution</h1> <p>When choosing a model, it might be tempting to select the model with the minimum loss. After all, if our model matches our data perfectly, why wouldn’t we use it? Unfortunately, this leads to the problem of <em>overfitting</em>, the issue where the model adheres too closely to the data we possess. Overfitting can occur for several reasons:</p> <ul> <li> <p>Insufficient data: the data doesn’t capture enough variation. If we roll a die 6 times and all of them happen to be exclusively odd numbers, then our model might think we have 0 probability of getting an even number.</p> </li> <li> <p>Noise: we might mistake noise to be a property of the data. Perhaps, because of high noise, we might want to use a high-degree polynomial regression to decrease the loss, even though a linear model would work perfectly fine.</p> </li> <li> <p>Irrelevant features: our model might rely on correlated features that have little to do with the output. For example, if a teacher’s socks are red and it tends to rain, we still wouldn’t care about the teacher’s sock color since it’s absurd to think the two are related (unless the teacher chooses to wear red socks on rainy days).</p> </li> </ul> <p>To prevent overfitting, we need to make decisions about what data is relevant to our problem and how much we should believe in our judgment.</p> <h1 id="train-validate-test">Train Validate Test</h1> <p>To calculate our model’s performance, we often split our data into a training set, validation set, and test set. The training set is used to train a model, the validation set is used to calculate the performance of a model, and the test set is used to measure the error in our model. The idea is that we train different models with the training set, compare them based on the error of the validation set (called the <em>validation error</em>), and finally select a model (usually the one with the minimum validation erorr), reporting its error on the test set (the <em>test error</em>). Notice that we do not look at the test set until the model has been selected. Our test set is meant to simulate the performance of the model on new, unforeseen data. If we were to evaluate the performance of our model on our test set, we would essentially be cheating on the efficacy of our model. This is known as <em>peeking</em> and should be avoided. The error in the test error is an unbiased estimate of the <em>generalization error</em>, the prediction accuracy on unseen data, whereas the validation error is a biased estimate of the generalization error.</p> <p>When considering how exactly to split your data, there are two factors to keep in mind. If your training set is too small, then none of your models will learn very well. However, if your validation set is too small, then the performance statistic will have high variance. A safe split of the data would be 60% train, 20% validation, 20% test.</p> <h1 id="k-fold-cross-validation">K-Fold Cross Validation</h1> <p>Sometimes, you might not have enough data to split your data into the three groups. One such solution to this problem would be to perform <em>k-fold cross validation</em>. The idea is you split your data into $k$ partitions, assign $k-1$ groups to training set, assign the remaining group to the validation set, and measure your model’s accuracy. The average of your accuracies over the $k$ different configurations would be your model’s validation error. This is a standard procedure, even though the experiments now lose independence.</p> <h1 id="bias-variance-decomposition">Bias-Variance Decomposition</h1> <p>$K$-fold cross validation improves the generalizability of the model, but we’d then have to train our model $k$ times, one per data configuration, greatly increasing the model generation time. For a better method in improving generalization, we need to understand the tradeoff between bias and variance.</p> <p><em>Bias</em> is the systematic error, or error that arises because the model tends deviate from the observed data. <em>Variance</em> is the sensitivity of prediction, or how much influence a data point has on the model. Simpler models tend to <em>underfit</em> the data because they tend to deviate from the provided data (high bias) but are robust to outliers in the data (low variance). More complex models tend to <em>overfit</em> the data because they adhere to the data (low bias) but are sensitive to outliers in the data (high variance).</p> <p>The generalization error can be summed up as the following:</p> <center>generalization = bias + variance</center> <p>The right tradeoff between bias and variance depends on the amount of data. With more data, we can afford to use more complex models.</p> <h1 id="bias-variance-analysis">Bias-Variance Analysis</h1> <p>Here, we detail the mathematics behind the bias-variance decomposition. We’ll define the following variables:</p> <ul> <li> <p>$D$: the data as a random variable</p> </li> <li> <p>$h_D: \mathcal{X} \to \mathbb{R}$, the trained model</p> </li> <li> <p>$\mathbf{x}$, a new input feature vector</p> </li> <li> <p>$y$, a true target value as a random variable</p> </li> </ul> <p>We would like to calculate the generalization error for $\mathbf{x}$:</p> <script type="math/tex; mode=display">\mathbb{E}_{D, y|\mathbf{x}} [(y - h_D(\mathbf{x}))^2]</script> <p>This is the expectation of the residual error between the true $y$ and the model’s estimate $y$ with respect to the data and the true $y$.</p> <p>We now define the <em>true conditional mean</em>:</p> <script type="math/tex; mode=display">\bar{y} = \mathbb{E}_{y|\mathbf{x}} [y]</script> <p>We then do a bit of arithmetic:</p> <script type="math/tex; mode=display">\mathbb{E}_{D, y|\mathbf{x}} [(y - h_D(\mathbf{x}))^2] = \mathbb{E}_{D, y|\mathbf{x}}[(y-\bar{y}+\bar{y} - h_D(\mathbf{x}))^2]</script> <script type="math/tex; mode=display">=\mathbb{E}_{y|\mathbf{x}}[(y-\bar{y})^2]+\mathbb{E}_D[(\bar{y}-h_D(\mathbf{x}))^2] + 2\mathbb{E}_{D,y|\mathbf{x}}[(y-\bar{y})(\bar{y}-h_D(\mathbf{x}))]</script> <p>The last term can be written as:</p> <script type="math/tex; mode=display">2\mathbb{E}_{D, y|\mathbf{x}}[\bar{y} - h_D(\mathbf{x})] \cdot \mathbb{E}_{y|\mathbf{x}}[y-\bar{y}] = 2\mathbb{E}_{D, y|\mathbf{x}}[\bar{y} - h_D(\mathbf{x})] \cdot 0 = 0</script> <p>So the generalization error simplifies to:</p> <script type="math/tex; mode=display">=\underbrace{\mathbb{E}_{y|\mathbf{x}}[(y-\bar{y})^2]}_\text{noise}+\underbrace{\mathbb{E}_D[(\bar{y}-h_D(\mathbf{x}))^2]}_\text{bias+variance}</script> <p>Let’s now split up the bias/variance term. Let the <em>prediction mean</em> be:</p> <script type="math/tex; mode=display">\bar{h}(\mathbf{x}) = \mathbb{E}_D[h_D(\mathbf{x})]</script> <p>Using similar arithmetic from before, we get:</p> <script type="math/tex; mode=display">\mathbb{E}_D[(\bar{y}-h_D(\mathbf{x}))^2] = \mathbb{E}_D[(\bar{y}-\bar{h}(\mathbf{x}) + \bar{h}(\mathbf{x}) - h_D(\mathbf{x}))^2]</script> <script type="math/tex; mode=display">=\underbrace{(\bar{y} - \bar{h}(\mathbf{x}))^2}_\text{bias squared} + \underbrace{\mathbb{E}_D[(\bar{h}(\mathbf{x}) - h_D(\mathbf{x}))^2]}_\text{variance} + \underbrace{2\mathbb{E}_D[(\bar{y}-\bar{h}(\mathbf{x}))(\bar{h}(\mathbf{x}) - h_D(\mathbf{x}))]}_0</script> <p>Plugging this back into the generalization error, we get:</p> <script type="math/tex; mode=display">=\underbrace{\mathbb{E}_{y|\mathbf{x}}[(y-\bar{y})^2]}_\text{noise}+\underbrace{(\bar{y} - \bar{h}(\mathbf{x}))^2}_\text{bias squared} + \underbrace{\mathbb{E}_D[(\bar{h}(\mathbf{x}) - h_D(\mathbf{x}))^2]}_\text{variance}</script> <script type="math/tex; mode=display">=\text{noise}(\mathbf{x})+(\text{bias}(h(\mathbf{x}))^2+\text{Var}_D(h_D(\mathbf{x}))</script> <p>The expectation of this over $\mathbf{x}$ is the generalization error. In conclusion, the generalization error is a function of the noise, bias, and variance of the data. Notice that as $n \to \infty$, the variance falls, allowing us to reduce generalization error and use a more complex model.</p> <h1 id="regularization">Regularization</h1> <p>To control this tradeoff, we can incorporate a <em>regularization term</em> in the loss function. Regularization terms penalize loss based on the magnitude and number of weights. Take, for example, the least-squared loss:</p> <script type="math/tex; mode=display">\mathcal{L}(\mathbf{x}) = \sum_{i=1}^n (y_i - h(\mathbf{x}_i; \mathbf{w}))^2</script> <p>One such regularization term is known as <em>ridge regression</em> where we add the squared $\ell_2$ norm:</p> <script type="math/tex; mode=display">% <![CDATA[
||\mathbf{x}||^2_2 = (\ell_2(\mathbf{x}))^2 = (\sqrt{\mathbf{x} \cdot \mathbf{x}})^2 = <\mathbf{x}, \mathbf{x}> %]]></script> <p>So our new loss to minimize would be:</p> <script type="math/tex; mode=display">\min_{\mathbf{w}} \mathcal{L}(\mathbf{w}) + ||\mathbf{w}||^2_2</script> <p>Fortunately, ridge regression is convex and differentiable, so the loss function can be optimized by matrix inversion. However, the inversion could be expensive for a large number of features. This is not the case for the <em>LASSO regression</em> which uses the $\ell_1$ norm:</p> <script type="math/tex; mode=display">\ell_1(\mathbf{w}) = ||\mathbf{w}||_1 = \sum_{i}^{|\mathbf{w}|}|w_i|</script> <p>for the loss function:</p> <script type="math/tex; mode=display">\min_{\mathbf{w}} \mathcal{L}(\mathbf{w}) + ||\mathbf{w}||_1</script> <p>Instead, we can always use stochastic methods/gradient descent to optimize these loss functions.</p> <p>Note that we can use both regularization terms together or even add hyperparameter constants to adjust the strength of each like so:</p> <script type="math/tex; mode=display">\min_{\mathbf{w}} \mathcal{L}(\mathbf{w}) + \lambda \mathbf{w}^\top\mathbf{w}</script> <h1 id="ensemble-methods">Ensemble Methods</h1> <p>Finally, usually a decrease in variance results in an increase in bias, but we can also reduce the variance without increasing bias by using <em>ensemble methods.</em> The ensemble method simply trains multiple models and takes the average (for regression, majority vote for classification) of predictions. This is known as <em>bagging.</em> In <em>boosting</em>, we train data sequentially, giving more weight to incorrectly classified examples.</p> <h1 id="conclusion">Conclusion</h1> <p>Phew, that was a lot about understanding how to choose good models. Once we get to neural networks, we don’t have to worry about any of this. :)</p> <div class="blog-nav"> <div class="spacer"></div> <h1 class="invite">Categories</h1> <p class="desc"><a href="/blog/tech/">TECHNOLOGY</a></p> <p class="desc"><a href="/blog/datasci/">DATA SCIENCE</a></p> <p class="desc"><a href="/blog/problems/">PROBLEM SOLVING</a></p> <p class="desc"><a href="/blog/graphics/">GRAPHIC ART</a></p> <p class="desc"><a href="/blog/misc/">MISCELLANEOUS</a></p> <hr> <h1 class="invite"><a href="/blog/">Recent Posts</a></h1> <ul> <p class="text"><a href="/datasci/2018/01/15/probabilistic-classification/">01/15/2018<br>Binary Classification</a></p> <p class="text"><a href="/datasci/2018/01/12/classification-metrics/">01/12/2018<br>Binary Classification Metrics</a></p> <p class="text"><a href="/datasci/2018/01/09/linear-classification/">01/09/2018<br>Linear Classification and Perceptron</a></p> <p class="text"><a href="/datasci/2018/01/08/bayesian-regression/">01/08/2018<br>Linear Regression: Bayesian Approach, Normal Conjugacy</a></p> <p class="text"><a href="/datasci/2018/01/08/basis-functions/">01/08/2018<br>Nonlinearity: Basis Functions</a></p> <p class="text"><a href="/datasci/2018/01/06/model-selection/">01/06/2018<br>Model Selection</a></p> <p class="text"><a href="/datasci/2018/01/04/probabilistic-regression/">01/04/2018<br>Linear Regression: A Probabilistic Approach</a></p> <p class="text"><a href="/datasci/2017/12/30/linear-regression/">12/30/2017<br>Linear Regression: A Mathematical Approach</a></p> <p class="text"><a href="/misc/2017/12/20/curating/">12/20/2017<br>2017 Reflections: A Year of Curating</a></p> <p class="text"><a href="/datasci/2017/12/19/regression-knn/">12/19/2017<br>Introduction to Regression: K-Nearest Neighbors</a></p> <p class="text"><a href="/misc/2017/12/18/misc-intro/">12/18/2017<br>Welcome to my Miscellaneous Blog!</a></p> <p class="text"><a href="/technology/2017/12/18/arch-install-chromebook/">12/18/2017<br>A Definitive Arch Linux Install Guide for the Chromebook C720</a></p> <p class="text"><a href="/graphics/2017/12/15/fire-room/">12/15/2017<br>C4D: Fire Room BTS</a></p> <p class="text"><a href="/graphics/2017/10/01/volume-effector/">10/01/2017<br>C4D: Volume Effector</a></p> <p class="text"><a href="/problems/2017/09/18/max-sliding-window/">09/18/2017<br>Algorithms: Maximum Sliding Window</a></p> <p class="text"><a href="/datasci/2017/09/10/inference-coins/">09/10/2017<br>Introduction to Inference: Coins and Discrete Probability</a></p> <p class="text"><a href="/technology/2017/09/05/geemichael-2/">09/05/2017<br>geemichael 2.0</a></p> <p class="text"><a href="/graphics/2017/09/05/boole-unreliability/">09/05/2017<br>C4D: Unreliable Booles</a></p> <p class="text"><a href="/technology/2017/08/30/tech-intro/">08/30/2017<br>Welcome to my Tech Blog!</a></p> <p class="text"><a href="/problems/2017/08/30/problems-intro/">08/30/2017<br>Welcome to my Problem Solving Blog!</a></p> <p class="text"><a href="/graphics/2017/08/30/graphic-art-intro/">08/30/2017<br>Welcome to my Graphic Art Blog!</a></p> <p class="text"><a href="/datasci/2017/08/30/datasci-intro/">08/30/2017<br>Welcome to my Data Science Blog!</a></p> </ul> </div> <center> <p> <a class="link" href="/datasci/2018/01/04/probabilistic-regression/"> Previous: Linear Regression: A Probabilistic Approach </a> | <a class="link" href="/datasci/2018/01/08/basis-functions/"> Next: Nonlinearity: Basis Functions </a> </p> </center> </div> <div class="white-container" id="footer-container"> <div class="black-centering"> <p>Michael Ge is a digital artist and data-driven software engineer.</p> <ul> <li class="link"><a href="https://www.linkedin.com/in/michael-ge-a9041b9a/">LinkedIn</a></li> <li class="link"><a href="https://github.com/hahakumquat">Github</a></li> <li class="link"><a href="https://www.instagram.com/hahakumquat/">Instagram</a></li> </ul> <ul> <li>Tel: (626) 893-5895</li> <li>Email: michaelge@college.harvard.edu</li> <li>Resumé and references available upon request.</li> </ul> </div> </div> <script src="/assets/js/jquery.js"></script> <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                extensions: ["tex2jax.js"],
                jax: ["input/TeX", "output/HTML-CSS"],
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true
                },
                TeX: {
                    Macros: {
                        t: ['{\\text{#1}}', 1],
                        bold: ["{\\bf{#1}}",1],
                        mbb: ['{\\mathbb{#1}}', 1],
                        mcal: ['{\\mathcatl{#1}}', 1]
                    }
                },
                "HTML-CSS": { availableFonts: ["TeX"] }
            });
        </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> </body> </html>