<!DOCTYPE html>
<html>
    <head>
    <title>Michael Ge | Software Engineer | Graphic Artist</title>
    <meta charset="UTF-8">
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-112248037-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-112248037-1');
    </script>
    <link rel="stylesheet" type="text/css" href="/assets/css/post.css">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
</head>

    <body>        
        <div id="nav-container">
    <ul class="nav">
        
            <li class="navli"><a class="text nava" href="/">HOME</a></li>
        
            <li class="navli"><a class="text nava" href="/about">ABOUT ME</a></li>
        
            <li class="navli"><a class="text nava" href="/art">ART</a></li>
        
            <li class="navli"><a class="text nava" href="/code">CODE</a></li>
        
            <li class="navli"><a class="text nava" href="/blog">BLOG</a></li>
        
            <li class="navli"><a class="text nava" href="/contact">CONTACT</a></li>
        
    </ul>
</div>

        <div class="spacer"></div>
        <div class="blog-container">
            <h1 class="header">Binary Classification</h1>
            <p class="invite">01/14/2018</p>
            <p class="desc tags"><i>Tags:
                
                machine learning
                
                probability
                
                classification
                
            </i></p>
            <p>Recall that in a binary classification problems, we have an input feature vector and we’d like to classify it into one of two classes. We did this by minimizing reasonable loss functions based on activation functions. In this very long post, we’ll take a probabilistic approach to classification and detail the generative framework.</p>

<h1 id="generative-probabilistic-view">Generative Probabilistic View</h1>

<p>There are two main probabilistic views of a classification problem: the generative and discriminative models. For some intuition of the generative model, suppose we’re playing God and we’re trying to construct wolf and sheep from scratch. We might think there’s a particular distribution of wolf and sheep in the world that we would like to maintain. In terms of creating the animal, if we know that we’re creating a wolf, the probability that the animal has the sharp teeth goes up and the probability that the animal has wool goes down. We might find that wooly wolves are less likely than sharp-toothed wolves, and wooly sheep are more likely than sharp-toothed sheep. Finally, if we know there is only one wolf for every million sheep, we might find that the probability of generating a wolf with any characteristics extremely unlikely. Most importantly, note that we have a model to decide how to generate the animals: specifically the relative proportions I’d like to maintain and what the features of the animal are.</p>

<p>In the <em>generative</em> view, we model the problem as trying to calculate the joint probability of both the data and the outcome:</p>

<script type="math/tex; mode=display">p(\mathbf{x}, y) = \overbrace{p(\mathbf{x}|y)}^\text{class-conditional}\underbrace{p(y)}_\text{class}</script>

<p>The <em>class</em> probability is the probability that a datapoint is in a certain class $y$, while the <em>class-conditional</em> probability is the probability that a datapoint has a certain feature vector given the class.</p>

<p>Note, we’ll also change our output space slightly to be:</p>

<script type="math/tex; mode=display">\mathcal{y} = \{0, 1\}</script>

<p>This will be useful when expressing our class prior.</p>

<h1 id="class-prior">Class Prior</h1>
<p>In binary classification’s $p(y)$, we need to define a prior distribution over the two classes. Let $\theta$ be the number of units in class 1.</p>

<script type="math/tex; mode=display">p(y=1; \theta) = \theta, p(y=0; \theta) = 1-\theta</script>

<p>Because of our output space change, we can equivalently write this as:</p>

<script type="math/tex; mode=display">p(y) = \theta^y(1-\theta)^{1-y}</script>

<p>We will later find an estimate of $\theta$ using Bayesian methods.</p>

<h1 id="class-conditional-prior">Class-Conditional Prior</h1>

<p>The class-conditional distribution is whatever distribution that defines $\mathbf{x}$. Depending on your feature vector, there are many ways to model $\mathbf{x}$. If they’re all Normally distributed, you might consider a multivariate Gaussian to model the class-conditional:</p>

<script type="math/tex; mode=display">p(\mathbf{x}|y=0; \mathbf{\mu}_0, \mathbf{\Sigma}_0)</script>

<script type="math/tex; mode=display">p(\mathbf{x}|y=1; \mathbf{\mu}_1, \mathbf{\Sigma}_1)</script>

<p>Of course, keep in mind that choosing distributions that yield conjugacy with your class priors or otherwise make computation easier might be favorable, but you might introduce model bias.</p>

<p>We’ll begin by assuming each of the covariates in $\mathbf{x}$ come from binary, discrete spaces:</p>

<script type="math/tex; mode=display">p(\mathbf{x}|y; \mathbf{\pi}_0, \mathbf{\pi}_1)</script>

<p>We can think of each $\pi$ as being a $ | \mathbf{x} | \times 2$ matrix of probabilities, 2 because each of the features will be from a discrete set of binary values. These $\pi$ parameters will be learned from the data.</p>

<h1 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h1>

<p>We’ve defined a generative model and specified the parameters to learn. Now onto the learning. We calculate the MLE of the parameters as the values that minimize the negative log-likelihood of the data:</p>

<script type="math/tex; mode=display">\min_{\mathbf{\pi}_0, \mathbf{\pi}_1, \theta}\mathcal{L}(\mathbf{\pi}_0, \mathbf{\pi}_1, \theta) = \min_{\mathbf{\pi}_0, \mathbf{\pi}_1, \theta} - \sum_{i=1}^n \left(\ln p(y, \theta) + \ln p(\mathbf{x}_i|y; \mathbf{\pi}_0, \mathbf{\pi}_1)\right)</script>

<p>Again, depending on the specified class and class-conditional distributions, there may be conjugate results that would make this easy to calculate, or you might have to use gradient descent to optimize the loss. The minimization would yield the optimal parameters under the MLE setting.</p>

<h1 id="sentiment-analysis">Sentiment Analysis</h1>

<p>Let’s look at an example problem. One popular area in classification is <em>sentiment analysis.</em> Given a text document $\mathbf{x}$ and a vocabulary $V$, we would like to classify the document as being a positive or negative sentiment.</p>

<p>Here are two examples:</p>

<p><em>I loved that movie! The plot was so wonderful.</em> has a positive sentiment.</p>

<p><em>I hated that movie. There was no plot.</em> has a negative sentiment.</p>

<h2 id="feature-representation-multinoulli">Feature Representation: Multinoulli</h2>

<p>We can let $y=1$ be the label for a positive sentiment, and $y=0$ be the negative sentiment. Furthermore, our $\mathbf{x}$ text will be transformed into a binary vector with an indicator variable for each word in the dictionary:</p>

<script type="math/tex; mode=display">\phi(\mathbf{x}) = \underbrace{[1, 0, 1, 0, 0, \ldots, 0]}_{\{0, 1\}^{|V|}}</script>

<p>A 1 at index $i$ indicates the $i$th word in the dictionary exists in document $\mathbf{x}$. This is known as the multivariate form of the Bernoulli, or the Multinoulli.</p>

<h2 id="multinomial-distribution-and-mle">Multinomial Distribution and MLE</h2>

<p>We might want to consider the counts of different words rather than their existence alone. Instead of using the Multinoulli distribution which only flags whether a word is seen or not, we can use a generalization of the Binomial (also a generalization of the Multinoulli) called the Multinomial distribution.</p>

<script type="math/tex; mode=display">p(\mathbf{x}; \mathbf{\pi}) \propto \prod_{j=1}^m \pi_j^{x_j}</script>

<p>Let’s take a slight tangent and calculate the MLE over <em>a set of feature vectors (without considering classes)</em>. Using a monotonic log transformation:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
& \arg_{\pi \ge 0}\max \prod_{i=1}^n p(\mathbf{x}_i; \pi) \\
& = \arg_{\pi \ge 0}\max \sum_{i=1}^n \sum_{j=1}^m x_{ij} \ln \pi_j \\
& \text{ s.t. } \sum_{j=1}^m \pi_j = 1\end{align*} %]]></script>

<p>We can then optimize this using Lagrange multipliers:</p>

<script type="math/tex; mode=display">L(\mathbf{\pi}, \lambda) = \sum_{i=1}^n \sum_{j=1}^m x_{ij} \ln \pi_j + \lambda \left(1 - \sum_{j=1}^m \pi_j \right)</script>

<script type="math/tex; mode=display">\frac{\partial L(\pi, \lambda)}{\partial \pi_j} = \sum_{i=1}^n \frac{x_{ij}}{\pi_j} - \lambda = 0</script>

<script type="math/tex; mode=display">\pi_j = \sum_{i=1}^n \frac{x_{ij}}{\lambda}</script>

<script type="math/tex; mode=display">\frac{\partial}{\partial \lambda} L(\pi, \lambda) = 1 - \sum_{j=1}^m \pi_j</script>

<script type="math/tex; mode=display">\sum_{j=1}^m \pi_j = 1</script>

<script type="math/tex; mode=display">\sum_{i=1}^n \sum_{j=1}^m \frac{x_{ij}}{\lambda} = 1</script>

<script type="math/tex; mode=display">\lambda = \sum_{i=1}^n \sum_{j=1}^m x_{ij}</script>

<p>Finally, we have an expression for $\pi$:</p>

<script type="math/tex; mode=display">\hat{\pi}_{MLE} = \frac{\sum_{i=1}^n \mathbf{x}_i}{\sum_{i=1}^n \sum_{j=1}^m \mathbf{x}_{ij}} = \frac{\mathbf{X}^\top \mathbf{1}}{\mathbf{1X1}}</script>

<p>The intuition behind this formula is that the maximum likelihood estimator for $\pi$ is quite simply the proportion of times a word occurs out of all the words.</p>

<h1 id="multinomial-binary-class-naive-bayes">Multinomial Binary-Class Naive Bayes</h1>

<p>We took some time to understand the form of the parameters $\pi$ when calculating the MLE. Now, we’ll see what happens when we incorporate class probabilities. To do this, we’ll take the Bayesian approach, Naive Bayes.</p>

<p>Here, we define our generative model as:</p>

<script type="math/tex; mode=display">p(\mathbf{x}, y) = p(y; \theta) p(\mathbf{x} | y; \mathbf{\pi}_0, \mathbf{\pi}_1)</script>

<p>Our class probability is distributed Bernoulli:</p>

<script type="math/tex; mode=display">p(y; \theta) = \text{Bern}(y; \theta)</script>

<p>Working under the sentiment analysis problem where we’d only like to flag words that appear (instead of counting), our class-conditional distribution will be Multinoulli:</p>

<script type="math/tex; mode=display">p(\mathbf{x}|y; \pi_0, \pi_1) \propto \prod_{j=1}^m \pi_{yj}^{x_j}</script>

<p>We can then calculate our desired probabilities of a datapoint being in a certain class given its features using Bayes’ Rule:</p>

<script type="math/tex; mode=display">p(y|\mathbf{x}) \propto p(\mathbf{x}|y; \pi_0, \pi_1) p(y; \theta)</script>

<h1 id="multinomial-naive-bayes-loss">Multinomial Naive Bayes Loss</h1>

<p>In a statistical context, our loss function will be to maximize the likelihood of the data:</p>

<script type="math/tex; mode=display">\max_{\theta; \mathbf{\pi}_0, \mathbf{\pi}_1} \sum_{i=1}^n \ln p(\mathbf{x}_i, y_i) = \max_{\mathbf{\pi}_0, \mathbf{\pi}_1} \sum_{i=1}^n \ln p(\mathbf{x}_i|y_i; \mathbf{\pi}_0, \mathbf{\pi}_1) + \max_\theta \sum_{i=1}^n \ln p(y_i; \theta)</script>

<p>After a <em>bunch</em> of algebra, we get to the conclusion that:</p>

<script type="math/tex; mode=display">\hat{\theta} = \frac{\mathbf{1y}}{n}</script>

<script type="math/tex; mode=display">\hat{\pi}_{0} = \frac{\sum_{i=1}^n (1-y_i)\mathbf{x}_i}{\sum_{i=1}^n \sum_{j=1}^m (1-y_i) x_{ij}}</script>

<script type="math/tex; mode=display">\hat{\pi}_{1} = \frac{\sum_{i=1}^n y_i\mathbf{x}_i}{\sum_{i=1}^n \sum_{j=1}^m y_i x_{ij}}</script>

<p>Intuitively, <script type="math/tex">\hat{\theta}</script> is the proportion of samples in class 1, and each <script type="math/tex">\hat{\pi}</script> is the normalized cumulative count vector over the features. Thus, we have derived non-MLE methods of finding the parameters using priors.</p>

<h1 id="discriminant-probabilistic-view">Discriminant Probabilistic View</h1>

<p>In practice, we would like to have a function that, having learned the parameters, decides whether a sample belongs in class 0 or 1. To do this, we can define a discriminant function where a sample is in class 1 if the function returns a positive number and 0 if the number is negative:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
h(\mathbf{x}) &= (\ln p(\mathbf{x}|y=1) + \ln p(y=1)) - (\ln p(\mathbf{x}|y=0) + \ln p(y=0)) \\
&= \left[\ln \prod_{j=1}^m \pi_{1j}^{x_j} - \ln \prod_{j=1}^m \pi_{0j}^{x_j}  \right] + [\ln \theta - \ln (1 - \theta)]\\
&= \sum_{j=1}^m x_j \ln \frac{\pi_{1j}}{\pi_{0j}} + \ln \frac{\theta}{1-\theta} = \mathbf{x}^\top \left(\ln \frac{\pi_1}{\pi_0} \right) + \ln \frac{\theta}{1-\theta}
\end{align*} %]]></script>

<p>We’ve recovered the form of a linear regression! Indeed, the binary classification problem ultimately reduces to a linear regression. Since much of the machinery of the linear regression is computationally easier, people often work under the regression framework.</p>

<p>If we choose to do this, then under the probabilistic framework of the linear regression, we would be maximizing:</p>

<script type="math/tex; mode=display">\arg_\mathbf{w}\max \prod_{i=1}^n p(y_i | \mathbf{x}_i, \mathbf{w})</script>

<p>Notice that here, we are parameterizing the class-conditional distribution. This is known as the <em>discriminative</em> model. Note that this is different from the generative model since we are not parameterizing the entire joint distribution. The benefits of doing so is that we are only learning weights to discriminate between classes, whereas in the generative model, we are learning weights about proportions and class-conditional frequencies. However, in the generative framework, we have a way of randomly generating <em>new data</em>, whereas in the discriminative model, we do not have this possibility since we did not learn the parameters required to generate data.</p>

<p>In the next post, we’ll talk more about the discriminative model and its relationship to the logistic regression.</p>

            <div class="blog-nav">
    <div class="spacer"></div>
    <h1 class="invite">Categories</h1>
    
    <p class="desc"><a href="/blog/tech/">TECHNOLOGY</a></p>
    
    <p class="desc"><a href="/blog/datasci/">DATA SCIENCE</a></p>
    
    <p class="desc"><a href="/blog/problems/">PROBLEM SOLVING</a></p>
    
    <p class="desc"><a href="/blog/graphics/">GRAPHIC ART</a></p>
    
    <p class="desc"><a href="/blog/misc/">MISCELLANEOUS</a></p>
    
    <hr>
    <h1 class="invite"><a href="/blog/">Recent Posts</a></h1>
    <ul>        
        
        
        
        <p class="text"><a href="/technology/2018/02/28/pacman-file-conflicts/">02/28/2018<br>Pacman: File Conflicts</a></p>
        
        
        
        <p class="text"><a href="/technology/2018/01/17/arch-usb/">01/17/2018<br>Making an Arch Linux USB Flash Install Medium</a></p>
        
        
        
        <p class="text"><a href="/technology/2018/01/17/arch-post-install/">01/17/2018<br>Arch Linux: Post-Install Notes</a></p>
        
        
        
        <p class="text"><a href="/datasci/2018/01/15/classification-metrics/">01/15/2018<br>Binary Classification Metrics</a></p>
        
        
        
        <p class="text"><a href="/datasci/2018/01/14/probabilistic-classification/">01/14/2018<br>Binary Classification</a></p>
        
        
        
        <p class="text"><a href="/datasci/2018/01/09/linear-classification/">01/09/2018<br>Linear Classification and Perceptron</a></p>
        
        
        
        <p class="text"><a href="/datasci/2018/01/08/bayesian-regression/">01/08/2018<br>Linear Regression: Bayesian Approach, Normal Conjugacy</a></p>
        
        
        
        <p class="text"><a href="/datasci/2018/01/08/basis-functions/">01/08/2018<br>Nonlinearity: Basis Functions</a></p>
        
        
        
        <p class="text"><a href="/datasci/2018/01/06/model-selection/">01/06/2018<br>Model Selection</a></p>
        
        
        
        <p class="text"><a href="/datasci/2018/01/04/probabilistic-regression/">01/04/2018<br>Linear Regression: A Probabilistic Approach</a></p>
        
        
        
        <p class="text"><a href="/datasci/2017/12/30/linear-regression/">12/30/2017<br>Linear Regression: A Mathematical Approach</a></p>
        
        
        
        <p class="text"><a href="/misc/2017/12/20/curating/">12/20/2017<br>2017 Reflections: A Year of Curating</a></p>
        
        
        
        <p class="text"><a href="/datasci/2017/12/19/regression-knn/">12/19/2017<br>Introduction to Regression: K-Nearest Neighbors</a></p>
        
        
        
        <p class="text"><a href="/misc/2017/12/18/misc-intro/">12/18/2017<br>Welcome to my Miscellaneous Blog!</a></p>
        
        
        
        <p class="text"><a href="/technology/2017/12/18/arch-install-chromebook/">12/18/2017<br>A Definitive Arch Linux Install Guide for the Chromebook C720</a></p>
        
        
        
        <p class="text"><a href="/graphics/2017/12/15/fire-room/">12/15/2017<br>C4D: Fire Room BTS</a></p>
        
        
        
        <p class="text"><a href="/graphics/2017/10/01/volume-effector/">10/01/2017<br>C4D: Volume Effector</a></p>
        
        
        
        <p class="text"><a href="/problems/2017/09/18/max-sliding-window/">09/18/2017<br>Algorithms: Maximum Sliding Window</a></p>
        
        
        
        <p class="text"><a href="/datasci/2017/09/10/inference-coins/">09/10/2017<br>Introduction to Inference: Coins and Discrete Probability</a></p>
        
        
        
        <p class="text"><a href="/technology/2017/09/05/geemichael-2/">09/05/2017<br>geemichael 2.0</a></p>
        
        
        
        <p class="text"><a href="/graphics/2017/09/05/boole-unreliability/">09/05/2017<br>C4D: Unreliable Booles</a></p>
        
        
        
        <p class="text"><a href="/technology/2017/08/30/tech-intro/">08/30/2017<br>Welcome to my Tech Blog!</a></p>
        
        
        
        <p class="text"><a href="/problems/2017/08/30/problems-intro/">08/30/2017<br>Welcome to my Problem Solving Blog!</a></p>
        
        
        
        <p class="text"><a href="/graphics/2017/08/30/graphic-art-intro/">08/30/2017<br>Welcome to my Graphic Art Blog!</a></p>
        
        
        
        <p class="text"><a href="/datasci/2017/08/30/datasci-intro/">08/30/2017<br>Welcome to my Data Science Blog!</a></p>
        
        
        
    </ul>
</div>

            <center>
                <p>
                    <a class="link" href="/datasci/2018/01/09/linear-classification/">
                        
                        Previous: Linear Classification and Perceptron
                        
                    </a>
                     | 
                    <a class="link" href="/datasci/2018/01/15/classification-metrics/">                        
                        
                        Next: Binary Classification Metrics
                        
                    </a>
                </p>
            </center>
        </div>        
        <div class="white-container" id="footer-container">
    <div class="black-centering">
        <p>Michael Ge is a digital artist and data-driven software engineer.</p>
        <ul>
            <li class="link"><a href="https://www.linkedin.com/in/michael-ge-a9041b9a/">LinkedIn</a></li>
            <li class="link"><a href="https://github.com/hahakumquat">Github</a></li>
            <li class="link"><a href="https://www.instagram.com/hahakumquat/">Instagram</a></li>
        </ul>
        <ul>
            <li>Tel: (626) 893-5895</li>
            <li>Email: michaelge@college.harvard.edu</li>
            <li>Resumé and references available upon request.</li>
        </ul>
    </div>
</div>

        <script src="/assets/js/jquery.js"></script>
        
        
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                extensions: ["tex2jax.js"],
                jax: ["input/TeX", "output/HTML-CSS"],
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true
                },
                TeX: {
                    Macros: {
                        t: ['{\\text{#1}}', 1],
                        bold: ["{\\bf{#1}}",1],
                        mbb: ['{\\mathbb{#1}}', 1],
                        mcal: ['{\\mathcatl{#1}}', 1]
                    }
                },
                "HTML-CSS": { availableFonts: ["TeX"] }
            });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
        
    </body>
</html>
