<!DOCTYPE html> <html> <head> <title>Michael Ge | Software Engineer | Graphic Artist</title> <meta charset="UTF-8"> <link rel="stylesheet" type="text/css" href="/assets/css/post.css"> <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico"> </head> <body> <div id="nav-container"> <ul class="nav"> <li class="navli"><a class="text nava" href="/">HOME</a></li> <li class="navli"><a class="text nava" href="/about">ABOUT ME</a></li> <li class="navli"><a class="text nava" href="/art">ART</a></li> <li class="navli"><a class="text nava" href="/code">CODE</a></li> <li class="navli"><a class="text nava" href="/blog">BLOG</a></li> <li class="navli"><a class="text nava" href="/contact">CONTACT</a></li> </ul> </div> <div class="spacer"></div> <div class="blog-container"> <h1 class="header">Linear Regression: A Mathematical Approach</h1> <h1>12/30/2017</h1> <p class="desc tags"><i>Tags: machine learning linear regression </i></p> <p>In this post, we’ll take a look at linear regression from a mathematical lens, ignoring the statistical interpretation. Here, we provide the derivation and interpretation of the closed form solution for the weights.</p> <p>In contrast with non-parametric regression approaches like K-Nearest Neighbors where an estimation is made simply by referring to other datapoints, parametric regression uses the data to optimize parameters (also known as weights) $\mathbf{w}$. These weights are then used to make future predictions. Be sure to read my blog post on my introduction to regression and KNN if that’s not immediately clear.</p> <p>Parametric modeling can be broken down into three main steps:</p> <ol> <li>Defining a loss criterion $\mathcal{L}$.</li> <li>Choosing a hypothesis $h$ with parameters $\mathbf{w}$.</li> <li>Optimizing $\mathbf{w}^\star$ by minimizing the loss function $\mathcal{L}$.</li> </ol> <p>To formalize this idea, we define a hypothesis function $\hat{y} = h(\mathbf{x}; \mathbf{w})$ where an estimate of a true $y$, $\hat{y}$ is a function of a feature vector $\mathbf{x}$ and given fixed parameters $\mathbf{w}$. To choose a specific $\mathbf{w^\star}$, we will then select a loss function $\mathcal{L}(\mathbf{w})$ such that the minimum of the loss suggests the best fit to the data.</p> <p>In linear regression, we define $h(\mathbf{x}; \mathbf{w})$ to be a linear combination of the features with an added bias parameter $w_0$:</p> <script type="math/tex; mode=display">h(\mathbf{x}; \mathbf{w}, w_0) = w_0 + \sum_{i=1}^m x_i w_i = \mathbf{w^\top x} + w_0</script> <script type="math/tex; mode=display">\mathbf{x}, \mathbf{w} \in \mathbb{R}^m, w_0 \in \mathbb{R}</script> <p>For notational simplicity, we often implicitly include the bias term $w_0$ in $\mathbf{w}$ and prepend a 1 to the feature vector. We will instead write the above expression assuming the bias term is within the weight vector:</p> <script type="math/tex; mode=display">\hat{y} = h(\mathbf{x^1}; \mathbf{w^1}, w_0) \to h(\mathbf{x}; \mathbf{w}) = \sum_{i=1}^{m+1} x_i w_i = \mathbf{w^\top x}</script> <script type="math/tex; mode=display">\mathbf{x}, \mathbf{w} \in \mathbb{R}^{m+1}, \mathbf{x} = \begin{bmatrix}1 \\ \mathbf{x^1}\end{bmatrix}, \mathbf{w} = \begin{bmatrix}w_0 \\ \mathbf{w^1} \end{bmatrix}</script> <p>There are many loss functions we can use, but one common loss function is the least squares loss. This is also known as the sum-of-squares residuals or error, SSR/SSE:</p> <script type="math/tex; mode=display">\mathcal{L}(\mathbf{w}) = \sum_{i=1}^n (y_i - \hat{y_i})^2 = \sum_{i=1}^n (y_i - \mathbf{w}^\top \mathbf{x}_i)^2</script> <p>One reason why we use least squares is that the function is convex. This allows us to solve for the optimum by using matrix calculus! We’ll write the derivative with respect to a vector as the following:</p> <script type="math/tex; mode=display">\frac{\partial g(\mathbf{z})}{\partial \mathbf{z}} = \begin{bmatrix}\frac{\partial g(z)}{\partial z_1} \\ \vdots \\ \frac{\partial g(z)}{z_m}\end{bmatrix}</script> <p>From this definition, it should be relatively straightforward to derive any of the math done here. To find the $\mathbf{w^\star}$ that yields the minimum loss, we can simply take the derivative of the loss with respect to $\mathbf{w}$ and solve for 0. The solution of $\mathbf{w}$ will be the parameter vector that minimizes the loss function.</p> <script type="math/tex; mode=display">\frac{\partial \mathcal{L}(\mathbf{w})}{\partial \mathbf{w}} = \sum_{i=1}^n -2(y_i - \mathbf{w}^\top\mathbf{x_i}) \mathbf{x}_i</script> <script type="math/tex; mode=display">= -2 \sum_{i=1}^n y_i\mathbf{x}_i + 2\sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^\top \cdot \mathbf{w}</script> <p>That’s pretty ugly. It looks a lot better if you work through this using matrix math. The matrix equivalent of the loss function is:</p> <script type="math/tex; mode=display">\mathcal{L}(\mathbf{w}) = (\mathbf{y} - \mathbf{Xw})^\top(\mathbf{y} - \mathbf{Xw})</script> <script type="math/tex; mode=display">=\mathbf{y}^\top\mathbf{y} - \mathbf{y}^\top\mathbf{Xw} - \mathbf{w}^\top\mathbf{X}^\top\mathbf{y} + \mathbf{w}^\top \mathbf{X}^\top \mathbf{X} \mathbf{w}</script> <p>These are all scalars. The transpose of a scalar is the same scalar, so the middle two terms are equal.</p> <script type="math/tex; mode=display">=\mathbf{y}^\top\mathbf{y} - 2\mathbf{w}^\top\mathbf{X}^\top\mathbf{y} + \mathbf{w}^\top \mathbf{X}^\top \mathbf{X} \mathbf{w}</script> <p>Let’s take a derivative:</p> <script type="math/tex; mode=display">\frac{\partial \mathcal{L}(\mathbf{w})}{\partial \mathbf{w}} = -2 \mathbf{X}^\top \mathbf{y} + (\mathbf{X}^\top \mathbf{X} + (\mathbf{X}^\top \mathbf{X})^\top)\mathbf{w}</script> <script type="math/tex; mode=display">\frac{\partial \mathcal{L}(\mathbf{w})}{\partial \mathbf{w}} = -2 \mathbf{X}^\top \mathbf{y} + 2\mathbf{X}^\top \mathbf{X}\mathbf{w}</script> <script type="math/tex; mode=display">\mathbf{w^\star} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}</script> <p>This is the linear algebra definition of a projection of $\mathbf{y}$ onto the column space of $\mathbf{X}$! Thus, the optimal regression line is defined by the projection of your data onto the feature space. In the next post on linear regression, we’ll be extending this interpretation into a stastical perspective.</p> <div class="blog-nav"> <div class="spacer"></div> <h1 class="invite">Categories</h1> <p class="desc"><a href="/blog/tech/">TECHNOLOGY</a></p> <p class="desc"><a href="/blog/datasci/">DATA SCIENCE</a></p> <p class="desc"><a href="/blog/problems/">PROBLEM SOLVING</a></p> <p class="desc"><a href="/blog/graphics/">GRAPHIC ART</a></p> <p class="desc"><a href="/blog/misc/">MISCELLANEOUS</a></p> <hr> <h1 class="invite"><a href="/blog/">Recent Posts</a></h1> <ul> <p class="text"><a href="/datasci/2018/01/09/linear-classification/">01/09/2018<br>Linear Classification and Perceptron</a></p> <p class="text"><a href="/datasci/2018/01/08/bayesian-regression/">01/08/2018<br>Linear Regression: Bayesian Approach, Normal Conjugacy</a></p> <p class="text"><a href="/datasci/2018/01/08/basis-functions/">01/08/2018<br>Nonlinearity: Basis Functions</a></p> <p class="text"><a href="/datasci/2018/01/06/model-selection/">01/06/2018<br>Model Selection</a></p> <p class="text"><a href="/datasci/2018/01/04/probabilistic-regression/">01/04/2018<br>Linear Regression: A Probabilistic Approach</a></p> <p class="text"><a href="/datasci/2017/12/30/linear-regression/">12/30/2017<br>Linear Regression: A Mathematical Approach</a></p> <p class="text"><a href="/misc/2017/12/20/curating/">12/20/2017<br>2017 Reflections: A Year of Curating</a></p> <p class="text"><a href="/datasci/2017/12/19/regression-knn/">12/19/2017<br>Introduction to Regression: K-Nearest Neighbors</a></p> <p class="text"><a href="/misc/2017/12/18/misc-intro/">12/18/2017<br>Welcome to my Miscellaneous Blog!</a></p> <p class="text"><a href="/technology/2017/12/18/arch-install-chromebook/">12/18/2017<br>A Definitive Arch Linux Install Guide for the Chromebook C720</a></p> <p class="text"><a href="/graphics/2017/12/15/fire-room/">12/15/2017<br>C4D: Fire Room BTS</a></p> <p class="text"><a href="/graphics/2017/10/01/volume-effector/">10/01/2017<br>C4D: Volume Effector</a></p> <p class="text"><a href="/problems/2017/09/18/max-sliding-window/">09/18/2017<br>Algorithms: Maximum Sliding Window</a></p> <p class="text"><a href="/datasci/2017/09/10/inference-coins/">09/10/2017<br>Introduction to Inference: Coins and Discrete Probability</a></p> <p class="text"><a href="/technology/2017/09/05/geemichael-2/">09/05/2017<br>geemichael 2.0</a></p> <p class="text"><a href="/graphics/2017/09/05/boole-unreliability/">09/05/2017<br>C4D: Unreliable Booles</a></p> <p class="text"><a href="/technology/2017/08/30/tech-intro/">08/30/2017<br>Welcome to my Tech Blog!</a></p> <p class="text"><a href="/problems/2017/08/30/problems-intro/">08/30/2017<br>Welcome to my Problem Solving Blog!</a></p> <p class="text"><a href="/graphics/2017/08/30/graphic-art-intro/">08/30/2017<br>Welcome to my Graphic Art Blog!</a></p> <p class="text"><a href="/datasci/2017/08/30/datasci-intro/">08/30/2017<br>Welcome to my Data Science Blog!</a></p> </ul> </div> <center> <p> <a class="link" href="/misc/2017/12/20/curating/"> Previous: 2017 Reflections: A Year of Curating </a> | <a class="link" href="/datasci/2018/01/04/probabilistic-regression/"> Next: Linear Regression: A Probabilistic Approach </a> </p> </center> </div> <div class="white-container" id="footer-container"> <div class="black-centering"> <p>Michael Ge is a digital artist and data-driven software engineer.</p> <ul> <li class="link"><a href="https://www.linkedin.com/in/michael-ge-a9041b9a/">LinkedIn</a></li> <li class="link"><a href="https://github.com/hahakumquat">Github</a></li> <li class="link"><a href="https://www.instagram.com/hahakumquat/">Instagram</a></li> </ul> <ul> <li>Tel: (626) 893-5895</li> <li>Email: michaelge@college.harvard.edu</li> <li>Resumé and references available upon request.</li> </ul> </div> </div> <script src="/assets/js/jquery.js"></script> <script src="https://d3js.org/d3.v4.min.js"></script> <script src="/assets/js/vis/knn.js"></script> <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                extensions: ["tex2jax.js"],
                jax: ["input/TeX", "output/HTML-CSS"],
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true
                },
                TeX: {
                    Macros: {
                        t: ['{\\text{#1}}', 1],
                        bold: ["{\\bf{#1}}",1],
                        mbb: ['{\\mathbb{#1}}', 1],
                        mcal: ['{\\mathcatl{#1}}', 1]
                    }
                },
                "HTML-CSS": { availableFonts: ["TeX"] }
            });
        </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> </body> </html>