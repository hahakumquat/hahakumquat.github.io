<!DOCTYPE html> <html> <head> <title>Michael Ge | Software Engineer | Graphic Artist</title> <meta charset="UTF-8"> <link rel="stylesheet" type="text/css" href="/assets/css/blog_list.css"> <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico"> </head> <body> <div id="nav-container"> <ul class="nav"> <li class="navli"><a class="text nava" href="/">HOME</a></li> <li class="navli"><a class="text nava" href="/about">ABOUT ME</a></li> <li class="navli"><a class="text nava" href="/art">ART</a></li> <li class="navli"><a class="text nava" href="/code">CODE</a></li> <li class="navli"><a class="text nava" href="/blog">BLOG</a></li> <li class="navli"><a class="text nava" href="/contact">CONTACT</a></li> </ul> </div> <div class="spacer"></div> <div class="blog-nav"> <div class="spacer"></div> <h1 class="invite">Blogs</h1> <ul> <p class="desc"><a href="/blog/tech/">TECHNOLOGY</a></p> <p class="desc"><a href="/blog/datasci/">DATA SCIENCE</a></p> <p class="desc"><a href="/blog/problems/">PROBLEM SOLVING</a></p> <p class="desc"><a href="/blog/graphics/">GRAPHIC ART</a></p> <p class="desc"><a href="/blog/misc/">MISCELLANEOUS</a></p> <hr> </ul> <ul> <h1 class="invite">Recent Posts</h1> <p class="text"><a href="/datasci/2018/01/08/bayesian-regression/">01/08/2018<br>Linear Regression: Bayesian Approach</a></p> <p class="text"><a href="/datasci/2018/01/06/model-selection/">01/06/2018<br>Model Selection</a></p> <p class="text"><a href="/datasci/2018/01/04/probabilistic-regression/">01/04/2018<br>Linear Regression: A Probabilistic Approach</a></p> <p class="text"><a href="/datasci/2017/12/30/linear-regression/">12/30/2017<br>Linear Regression: A Mathematical Approach</a></p> <p class="text"><a href="/misc/2017/12/20/curating/">12/20/2017<br>2017 Reflections: A Year of Curating</a></p> <p class="text"><a href="/datasci/2017/12/19/regression-knn/">12/19/2017<br>Introduction to Regression: K-Nearest Neighbors</a></p> <p class="text"><a href="/misc/2017/12/18/misc-intro/">12/18/2017<br>Welcome to my Miscellaneous Blog!</a></p> <p class="text"><a href="/technology/2017/12/18/arch-install-chromebook/">12/18/2017<br>A Definitive Arch Linux Install Guide for the Chromebook C720</a></p> <p class="text"><a href="/graphics/2017/12/15/fire-room/">12/15/2017<br>C4D: Fire Room BTS</a></p> <p class="text"><a href="/graphics/2017/10/01/volume-effector/">10/01/2017<br>C4D: Volume Effector</a></p> <p class="text"><a href="/problems/2017/09/18/max-sliding-window/">09/18/2017<br>Algorithms: Maximum Sliding Window</a></p> <p class="text"><a href="/datasci/2017/09/10/inference-coins/">09/10/2017<br>Introduction to Inference: Coins and Discrete Probability</a></p> <p class="text"><a href="/technology/2017/09/05/geemichael-2/">09/05/2017<br>geemichael 2.0</a></p> <p class="text"><a href="/graphics/2017/09/05/boole-unreliability/">09/05/2017<br>C4D: Unreliable Booles</a></p> <p class="text"><a href="/technology/2017/08/30/tech-intro/">08/30/2017<br>Welcome to my Tech Blog!</a></p> <p class="text"><a href="/problems/2017/08/30/problems-intro/">08/30/2017<br>Welcome to my Problem Solving Blog!</a></p> <p class="text"><a href="/graphics/2017/08/30/graphic-art-intro/">08/30/2017<br>Welcome to my Graphic Art Blog!</a></p> <p class="text"><a href="/datasci/2017/08/30/datasci-intro/">08/30/2017<br>Welcome to my Data Science Blog!</a></p> </ul> </div> <a href="/datasci/2018/01/08/bayesian-regression/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Linear Regression: Bayesian Approach</h1> <div class="info"> <p>01/08/2018</p> <p>tags: machine learning bayesian regression </p> </div> </div> <div class="container"> <p>Understanding the linear regression from a probabilistic perspective allows us to perform more advanced statistical inference. Today, we’ll be applying Bayesian inference concepts to the linear regression. As a result, we’ll have a way to update the beliefs of our models as more data becomes accessible or account for prior knowledge when looking at data.</p> </div> </div> </a> <a href="/datasci/2018/01/06/model-selection/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Model Selection</h1> <div class="info"> <p>01/06/2018</p> <p>tags: machine learning model selection overfitting </p> </div> </div> <div class="container"> <p>So far, we’ve looked at linear regression and K-Nearest Neighbors as potential models for estimating real-valued data. But how do we know which model is the best to use? In this post, we discuss overfitting, bias-variance decomposition, and regularization as factors when considering models.</p> </div> </div> </a> <a href="/datasci/2018/01/04/probabilistic-regression/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Linear Regression: A Probabilistic Approach</h1> <div class="info"> <p>01/04/2018</p> <p>tags: machine learning linear regression probability </p> </div> </div> <div class="container"> <p>In my <a class="link" href="/datasci/2017/12/30/linear-regression/">last post</a>, I discussed the mathematical intuition behind the linear regression. Today, we look at the regression under a probabilistic modeling context that help us understand the reasons behind the least squares loss function.</p> </div> </div> </a> <a href="/datasci/2017/12/30/linear-regression/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Linear Regression: A Mathematical Approach</h1> <div class="info"> <p>12/30/2017</p> <p>tags: machine learning linear regression </p> </div> </div> <div class="container"> <p>In this post, we’ll take a look at linear regression from a mathematical lens, ignoring the statistical interpretation. Here, we provide the derivation and interpretation of the closed form solution for the weights.</p> </div> </div> </a> <a href="/datasci/2017/12/19/regression-knn/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Introduction to Regression: K-Nearest Neighbors</h1> <div class="info"> <p>12/19/2017</p> <p>tags: machine learning k nearest neighbors </p> </div> </div> <div class="container"> <p>Here, we’ll look at the K-Nearest Neighbors approach toward understanding one of the core ideas of machine learning, the regression.</p> </div> </div> </a> <a href="/datasci/2017/09/10/inference-coins/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Introduction to Inference: Coins and Discrete Probability</h1> <div class="info"> <p>09/10/2017</p> <p>tags: discrete inference machine learning </p> </div> </div> <div class="container"> <p>In data science, it all starts with a coin. Today, we’ll talk about the fundamentals of statistical inference for discrete models: how to determine the optimal parameters given data, how to incorporate prior knowledge, and how to make predictions. This assumes familiarity with random variables and the basics of probability theory.</p> </div> </div> </a> <a href="/datasci/2017/08/30/datasci-intro/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Welcome to my Data Science Blog!</h1> <div class="info"> <p>08/30/2017</p> <p>tags: intro </p> </div> </div> <div class="container"> <p>Welcome to my data science blog! Here, you’ll find posts about the various things I’m working on as well as tips and insights I’ve gained during the project.</p> </div> </div> </a> <div class="white-container" id="footer-container"> <div class="black-centering"> <p>Michael Ge is a digital artist and data-driven software engineer.</p> <ul> <li class="link"><a href="https://www.linkedin.com/in/michael-ge-a9041b9a/">LinkedIn</a></li> <li class="link"><a href="https://github.com/hahakumquat">Github</a></li> <li class="link"><a href="https://www.instagram.com/hahakumquat/">Instagram</a></li> </ul> <ul> <li>Tel: (626) 893-5895</li> <li>Email: michaelge@college.harvard.edu</li> <li>Resumé and references available upon request.</li> </ul> </div> </div> <script src="/assets/js/jquery.js"></script> </body> </html>