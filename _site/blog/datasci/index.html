<!DOCTYPE html> <html> <head> <title>Michael Ge | Software Engineer | Graphic Artist</title> <meta charset="UTF-8"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-112248037-1"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-112248037-1");</script> <link rel="stylesheet" type="text/css" href="/assets/css/blog_list.css"> <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico"> </head> <body> <div id="nav-container"> <ul class="nav"> <li class="navli"><a class="text nava" href="/">HOME</a></li> <li class="navli"><a class="text nava" href="/about">ABOUT ME</a></li> <li class="navli"><a class="text nava" href="/art">ART</a></li> <li class="navli"><a class="text nava" href="/code">CODE</a></li> <li class="navli"><a class="text nava" href="/blog">BLOG</a></li> <li class="navli"><a class="text nava" href="/contact">CONTACT</a></li> </ul> </div> <div class="spacer"></div> <div class="blog-nav"> <div class="spacer"></div> <h1 class="invite">Categories</h1> <p class="desc"><a href="/blog/tech/">TECHNOLOGY</a></p> <p class="desc"><a href="/blog/datasci/">DATA SCIENCE</a></p> <p class="desc"><a href="/blog/problems/">PROBLEM SOLVING</a></p> <p class="desc"><a href="/blog/graphics/">GRAPHIC ART</a></p> <p class="desc"><a href="/blog/misc/">MISCELLANEOUS</a></p> <hr> <h1 class="invite"><a href="/blog/">Recent Posts</a></h1> <ul> <p class="text"><a href="/datasci/2018/01/15/probabilistic-classification/">01/15/2018<br>Binary Classification</a></p> <p class="text"><a href="/datasci/2018/01/12/classification-metrics/">01/12/2018<br>Binary Classification Metrics</a></p> <p class="text"><a href="/datasci/2018/01/09/linear-classification/">01/09/2018<br>Linear Classification and Perceptron</a></p> <p class="text"><a href="/datasci/2018/01/08/bayesian-regression/">01/08/2018<br>Linear Regression: Bayesian Approach, Normal Conjugacy</a></p> <p class="text"><a href="/datasci/2018/01/08/basis-functions/">01/08/2018<br>Nonlinearity: Basis Functions</a></p> <p class="text"><a href="/datasci/2018/01/06/model-selection/">01/06/2018<br>Model Selection</a></p> <p class="text"><a href="/datasci/2018/01/04/probabilistic-regression/">01/04/2018<br>Linear Regression: A Probabilistic Approach</a></p> <p class="text"><a href="/datasci/2017/12/30/linear-regression/">12/30/2017<br>Linear Regression: A Mathematical Approach</a></p> <p class="text"><a href="/misc/2017/12/20/curating/">12/20/2017<br>2017 Reflections: A Year of Curating</a></p> <p class="text"><a href="/datasci/2017/12/19/regression-knn/">12/19/2017<br>Introduction to Regression: K-Nearest Neighbors</a></p> <p class="text"><a href="/misc/2017/12/18/misc-intro/">12/18/2017<br>Welcome to my Miscellaneous Blog!</a></p> <p class="text"><a href="/technology/2017/12/18/arch-install-chromebook/">12/18/2017<br>A Definitive Arch Linux Install Guide for the Chromebook C720</a></p> <p class="text"><a href="/graphics/2017/12/15/fire-room/">12/15/2017<br>C4D: Fire Room BTS</a></p> <p class="text"><a href="/graphics/2017/10/01/volume-effector/">10/01/2017<br>C4D: Volume Effector</a></p> <p class="text"><a href="/problems/2017/09/18/max-sliding-window/">09/18/2017<br>Algorithms: Maximum Sliding Window</a></p> <p class="text"><a href="/datasci/2017/09/10/inference-coins/">09/10/2017<br>Introduction to Inference: Coins and Discrete Probability</a></p> <p class="text"><a href="/technology/2017/09/05/geemichael-2/">09/05/2017<br>geemichael 2.0</a></p> <p class="text"><a href="/graphics/2017/09/05/boole-unreliability/">09/05/2017<br>C4D: Unreliable Booles</a></p> <p class="text"><a href="/technology/2017/08/30/tech-intro/">08/30/2017<br>Welcome to my Tech Blog!</a></p> <p class="text"><a href="/problems/2017/08/30/problems-intro/">08/30/2017<br>Welcome to my Problem Solving Blog!</a></p> <p class="text"><a href="/graphics/2017/08/30/graphic-art-intro/">08/30/2017<br>Welcome to my Graphic Art Blog!</a></p> <p class="text"><a href="/datasci/2017/08/30/datasci-intro/">08/30/2017<br>Welcome to my Data Science Blog!</a></p> </ul> </div> <a href="/datasci/2018/01/15/probabilistic-classification/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Binary Classification</h1> <div class="info"> <p class="invite">01/15/2018</p> <p>tags: machine learning probability classification </p> </div> </div> <img class="thumb" src=""/> <div class="container"> <p>Recall that in a binary classification problems, we have an input feature vector and we’d like to classify it into one of two classes. We did this by minimizing reasonable loss functions based on activation functions. In this very long post, we’ll take a probabilistic approach to classification and detail the generative framework.</p> </div> </div> </a> <a href="/datasci/2018/01/12/classification-metrics/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Binary Classification Metrics</h1> <div class="info"> <p class="invite">01/12/2018</p> <p>tags: machine learning accuracy error </p> </div> </div> <img class="thumb" src=""/> <div class="container"> <p>Depending on the situation, the simple “number of correct classifications” error metric might not be the best metric to use in binary classification. Here, we explore several metrics and how they might be used for different problems.</p> </div> </div> </a> <a href="/datasci/2018/01/09/linear-classification/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Linear Classification and Perceptron</h1> <div class="info"> <p class="invite">01/09/2018</p> <p>tags: machine learning classification perceptron </p> </div> </div> <img class="thumb" src=""/> <div class="container"> <p>We now leave the land of predicting real-numbered values to look at data classification. The discussion will conclude with one of the fundamental concepts behind classification, the Perceptron algorithm.</p> </div> </div> </a> <a href="/datasci/2018/01/08/bayesian-regression/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Linear Regression: Bayesian Approach, Normal Conjugacy</h1> <div class="info"> <p class="invite">01/08/2018</p> <p>tags: machine learning bayesian regression </p> </div> </div> <div class="container"> <p>Understanding the linear regression from a probabilistic perspective allows us to perform more advanced statistical inference. Today, we’ll be applying Bayesian inference concepts to the linear regression. As a result, we’ll have a way to update the beliefs of our models as more data becomes accessible or account for prior knowledge when looking at data.</p> </div> </div> </a> <a href="/datasci/2018/01/08/basis-functions/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Nonlinearity: Basis Functions</h1> <div class="info"> <p class="invite">01/08/2018</p> <p>tags: machine learning non-linearity basis functions </p> </div> </div> <img class="thumb" src=""/> <div class="container"> <p>We often work in linear space, but you might ask how we could capture nonlinearity? The answer lies in basis functions.</p> </div> </div> </a> <a href="/datasci/2018/01/06/model-selection/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Model Selection</h1> <div class="info"> <p class="invite">01/06/2018</p> <p>tags: machine learning model selection overfitting </p> </div> </div> <div class="container"> <p>So far, we’ve looked at linear regression and K-Nearest Neighbors as potential models for estimating real-valued data. But how do we know which model is the best to use? In this post, we discuss overfitting, bias-variance decomposition, and regularization as factors when considering models.</p> </div> </div> </a> <a href="/datasci/2018/01/04/probabilistic-regression/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Linear Regression: A Probabilistic Approach</h1> <div class="info"> <p class="invite">01/04/2018</p> <p>tags: machine learning linear regression probability </p> </div> </div> <div class="container"> <p>Today, we look at the regression under a probabilistic modeling context that help us understand the reasons behind the least squares loss function.</p> </div> </div> </a> <a href="/datasci/2017/12/30/linear-regression/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Linear Regression: A Mathematical Approach</h1> <div class="info"> <p class="invite">12/30/2017</p> <p>tags: machine learning linear regression </p> </div> </div> <div class="container"> <p>In this post, we’ll take a look at linear regression from a mathematical lens, ignoring the statistical interpretation. Here, we provide the derivation and interpretation of the closed form solution for the weights.</p> </div> </div> </a> <a href="/datasci/2017/12/19/regression-knn/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Introduction to Regression: K-Nearest Neighbors</h1> <div class="info"> <p class="invite">12/19/2017</p> <p>tags: machine learning k nearest neighbors </p> </div> </div> <div class="container"> <p>Here, we’ll look at the K-Nearest Neighbors approach toward understanding one of the core ideas of machine learning, the regression.</p> </div> </div> </a> <a href="/datasci/2017/09/10/inference-coins/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Introduction to Inference: Coins and Discrete Probability</h1> <div class="info"> <p class="invite">09/10/2017</p> <p>tags: discrete inference machine learning </p> </div> </div> <div class="container"> <p>In data science, it all starts with a coin. Today, we’ll talk about the fundamentals of statistical inference for discrete models: how to determine the optimal parameters given data, how to incorporate prior knowledge, and how to make predictions. This assumes familiarity with random variables and the basics of probability theory.</p> </div> </div> </a> <a href="/datasci/2017/08/30/datasci-intro/"> <div class="blog-frame-container"> <div class="container"> <h1 class="header">Welcome to my Data Science Blog!</h1> <div class="info"> <p class="invite">08/30/2017</p> <p>tags: intro </p> </div> </div> <div class="container"> <p>Welcome to my data science blog! Here, you’ll find posts about the various things I’m working on as well as tips and insights I’ve gained during the project.</p> </div> </div> </a> <div class="white-container" id="footer-container"> <div class="black-centering"> <p>Michael Ge is a digital artist and data-driven software engineer.</p> <ul> <li class="link"><a href="https://www.linkedin.com/in/michael-ge-a9041b9a/">LinkedIn</a></li> <li class="link"><a href="https://github.com/hahakumquat">Github</a></li> <li class="link"><a href="https://www.instagram.com/hahakumquat/">Instagram</a></li> </ul> <ul> <li>Tel: (626) 893-5895</li> <li>Email: michaelge@college.harvard.edu</li> <li>Resumé and references available upon request.</li> </ul> </div> </div> <script src="/assets/js/jquery.js"></script> </body> </html>